{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from preprocessing import getSentenceData\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the different operation classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyGate:\n",
    "    def forward(self,W,x):\n",
    "        return np.dot(W,x)\n",
    "    def backward(self,W,x,dz):\n",
    "        dW = np.asarray(np.dot(np.transpose(np.asmatrix(dz)), np.asmatrix(x))) #upstream diffrential * other operand\n",
    "        dx = np.dot(np.transpose(W),dz)  \n",
    "        return dW, dx\n",
    "\n",
    "class AddGate:\n",
    "    def forward(self,x1,x2):\n",
    "        return x1 + x2\n",
    "    def backward(self,x1,x2,dz):\n",
    "        dx1 = dz * np.ones_like(x1) #Diff = upstream diff  \n",
    "        dx2 = dx * np.ones_like(x2)\n",
    "        return dx1,dx2\n",
    "        \n",
    "class sigmoid: \n",
    "    def forward(self,x):\n",
    "        return 1 / 1 + np.exp(-x)\n",
    "    def backward(self,x,top_diff):\n",
    "        output = self.forward(x)\n",
    "        return (1 - output) * output * top_diff\n",
    "        \n",
    "class Tanh:\n",
    "    def forward(self,x):\n",
    "        return np.tanh(x)\n",
    "    def backward(self,x,top_diff):\n",
    "        output = self.forward(x)\n",
    "        return (1 - np.square(output))*top_diff  \n",
    "\n",
    "class Softmax:\n",
    "    def predict(self,x):\n",
    "        exp_scores = np.exp(x)\n",
    "        return exp_scores / (np.sum(exp_scores))\n",
    "    def loss(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        return -np.log(probs[y])\n",
    "    def diff(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        probs[y] -= 1.0\n",
    "        return probs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    output = Softmax()\n",
    "    layers = self.forward_propagation(x)\n",
    "    return [np.argmax(output.predict(layer.mulv)) for layer in layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of RNN Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulGate = MultiplyGate()\n",
    "addGate = AddGate()\n",
    "activation = Tanh()\n",
    "\n",
    "#h = hidden state\n",
    "\n",
    "class RNNLayer: \n",
    "    def forward(self, x, prev_h, U, W, V):\n",
    "        self.mulu = mulGate.forward(U,x)\n",
    "        self.mulw = mulGate.forward(W, prev_h)\n",
    "        self.add = addGate.forward(self.mulu,self.mulw)\n",
    "        self.h = activation.forward(self.add)\n",
    "        self.mulv = mulGate.forward(V,self.h)\n",
    "        \n",
    "    def backward(self, x, prev_h, U, W, V, diff_h):\n",
    "        self.forward(x, prev_h, U, W, V)\n",
    "        dV, dsv = mulGate.backward(V, self.h, dmulv)\n",
    "        ds = dsv + diff_h\n",
    "        dadd = activation.backward(self.add, ds)\n",
    "        dmulw, dmulu = addGate.backward(self.mulw, self.mulu, dadd)\n",
    "        dW, dprev_h = mulGate.backward(W, prev_h, dmulw)\n",
    "        dU, dx = mulGate.backward(U, x, dmulu)\n",
    "        return (dprev_h, dU, dW, dV)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    word_dim = size of vocabulary \n",
    "    hidden_dim = number of hidden layers \n",
    "    n is the number of incoming connections from the previous layer\n",
    "'''\n",
    "\n",
    "class Model:\n",
    "    #Initialization\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.bptt_truncate = bptt_truncate \n",
    "        self.U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, word_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. /  hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. /  hidden_dim), (word_dim, hidden_dim))\n",
    "    \n",
    "    #Forward propagation \n",
    "    \n",
    "    def forward_propagation(self, x):\n",
    "        #x batch of word example x = [0, 145 ,256 ,532] the outcome could be [145, 256, 532, 1] with 0 = start and 1 = end\n",
    "        T = len(x) # Number of words in the batch\n",
    "        layers = []\n",
    "        prev_h = np.zeros(self.hidden_dim) \n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = RNNLayer()\n",
    "            entry = np.zeros(self.word_dim)\n",
    "            entry[x[t]] = 1  # One hot representation of a word \n",
    "            layer.forward(entry, prev_h, self.U, self.W, self.V)\n",
    "            prev_h = layer.h\n",
    "            layers.append(layer)\n",
    "            \n",
    "        return layers\n",
    "    \n",
    "    #Calculating the loss\n",
    "    \n",
    "    def calculate_loss(self,x ,y):\n",
    "        assert len(x) == len(y)\n",
    "        output = Softmax()\n",
    "        layers = self.forward_propagation(x)\n",
    "        loss = 0.0\n",
    "        for i,layer in enumerate(layers):\n",
    "            loss += output.loss(layer.mulv, y[i])\n",
    "        return loss / float(len(Y))\n",
    "    \n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        loss = 0.0\n",
    "        for i in range(len(Y)):\n",
    "            loss += self.calculate_loss(X[i], Y[i])\n",
    "        return loss / float(len(Y))\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        \n",
    "        assert len(x) == len(y)\n",
    "        output = Softmax()\n",
    "        layers = self.forward_propagation(x)\n",
    "        dU = np.zeros(self.U.shape)\n",
    "        dV = np.zeros(self.V.shape)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "\n",
    "        T = len(layers)\n",
    "        prev_s_t = np.zeros(self.hidden_dim)\n",
    "        diff_s = np.zeros(self.hidden_dim)\n",
    "        for t in range(0, T):\n",
    "            \n",
    "            dmulv = output.diff(layers[t].mulv, y[t])\n",
    "            entry = np.zeros(self.word_dim)\n",
    "            entry[x[t]] = 1\n",
    "            dprev_s, dU_t, dW_t, dV_t = layers[t].backward(entry, prev_s_t, self.U, self.W, self.V, diff_s, dmulv)\n",
    "            prev_s_t = layers[t].s\n",
    "            dmulv = np.zeros(self.word_dim)\n",
    "            for i in range(t-1, max(-1, t-self.bptt_truncate-1), -1):\n",
    "                input = np.zeros(self.word_dim)\n",
    "                input[x[i]] = 1\n",
    "                prev_s_i = np.zeros(self.hidden_dim) if i == 0 else layers[i-1].s\n",
    "                dprev_s, dU_i, dW_i, dV_i = layers[i].backward(input, prev_s_i, self.U, self.W, self.V, dprev_s, dmulv)\n",
    "                dU_t += dU_i\n",
    "                dW_t += dW_i\n",
    "            dV += dV_t\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "        return (dU, dW, dV)\n",
    "    \n",
    "    \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        dU, dW, dV = self.bptt(x, y)\n",
    "        self.U -= learning_rate * dU\n",
    "        self.V -= learning_rate * dV\n",
    "        self.W -= learning_rate * dW\n",
    "    \n",
    "    def train(self, X, Y, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "        num_examples_seen = 0\n",
    "        losses = []\n",
    "        for epoch in range(nepoch):\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = self.calculate_total_loss(X, Y)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "                # Adjust the learning rate if loss increases\n",
    "                if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "                    learning_rate = learning_rate * 0.5\n",
    "                    print(\"Setting learning rate to %f\" % learning_rate)\n",
    "                sys.stdout.flush()\n",
    "            # For each training example...\n",
    "            for i in range(len(Y)):\n",
    "                self.sgd_step(X[i], Y[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "        return losses \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79171 sentences.\n",
      "Found 65467 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'documentary' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n",
      "\n",
      "X_train shape: (78483,)\n",
      "y_train shape: (78483,)\n",
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[0, 51, 27, 16, 10, 857, 54, 25, 34, 69]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[51, 27, 16, 10, 857, 54, 25, 34, 69, 1]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "backward() takes 7 positional arguments but 8 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1ae1d53abd8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-ac34785d12c0>\u001b[0m in \u001b[0;36msgd_step\u001b[0;34m(self, x, y, learning_rate)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msgd_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbptt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-ac34785d12c0>\u001b[0m in \u001b[0;36mbptt\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mdprev_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdU_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_s_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdmulv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mprev_s_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mdmulv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: backward() takes 7 positional arguments but 8 were given"
     ]
    }
   ],
   "source": [
    "word_dim = 8000\n",
    "hidden_dim = 100\n",
    "X_train, y_train = getSentenceData('data/reddit-comments-2015-08.csv', word_dim)\n",
    "\n",
    "np.random.seed(10)\n",
    "rnn = Model(word_dim, hidden_dim)\n",
    "rnn.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "shape() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0f6b0b00763b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: shape() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
